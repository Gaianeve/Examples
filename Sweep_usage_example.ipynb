{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Wand Setup\n",
        "\n",
        "Start out by installing the experiment tracking library and setting up your free W&B account:\n",
        "\n",
        "1. Install with `!pip install`\n",
        "2. `import` the library into Python\n",
        "3. `.login()` so you can log metrics to your projects\n",
        "\n",
        "If you've never used Weights & Biases before,\n",
        "the call to `login` will give you a link to sign up for an account.\n",
        "W&B is free to use for personal and academic projects!"
      ],
      "metadata": {
        "id": "NYwPLQ5yK7P_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BrTloKeKlJC",
        "outputId": "e704aaf6-201a-4c9d-a401-5b60547effd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgaiabartoli7\u001b[0m (\u001b[33mcartpole_maria_gaia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install wandb -Uq\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1Ô∏è‚É£. Define the Sweep\n",
        "\n",
        "Fundamentally, a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them.\n",
        "Whether that strategy is as simple as trying every option\n",
        "or as complex as [BOHB](https://arxiv.org/abs/1807.01774),\n",
        "Weights & Biases Sweeps have you covered.\n",
        "You just need to _define your strategy_\n",
        "in the form of a [configuration](https://docs.wandb.com/sweeps/configuration).\n",
        "\n",
        "When you're setting up a Sweep in a notebook like this,\n",
        "that config object is a nested dictionary.\n",
        "When you run a Sweep via the command line,\n",
        "the config object is a\n",
        "[YAML file](https://docs.wandb.com/sweeps/quickstart#2-sweep-config).\n",
        "\n",
        "So, for now, no YAML file: it's only nedeed if u work in code line.\n"
      ],
      "metadata": {
        "id": "zYRCvYiCLa98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üëà Pick a `method`"
      ],
      "metadata": {
        "id": "9VkjpVRcMkLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to define is the `method`\n",
        "for choosing new parameter values.\n",
        "\n",
        "We provide the following search `methods`:\n",
        "*   **`grid` Search** ‚Äì Iterate over every combination of hyperparameter values.\n",
        "Very effective, but can be computationally costly.\n",
        "*   **`random` Search** ‚Äì Select each new combination at random according to provided `distribution`s. Surprisingly effective!\n",
        "*   **`bayes`ian Search** ‚Äì Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
        "\n",
        "We'll stick with `random`."
      ],
      "metadata": {
        "id": "c0JmuznjMkXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "\n",
        "#only necessary for bayes search, not needed for others. In our specific case we shall maximize the loss\n",
        "  metric = {\n",
        "  'name': 'loss',\n",
        "  'goal': 'minimize'\n",
        "  }\n",
        "\n",
        "sweep_config['metric'] = metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "32GQb3kZMkAT",
        "outputId": "e786cbcc-26eb-4924-f06c-ce9959d8c14a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-3-5431c12e29ad>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-5431c12e29ad>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    metric = {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "best strategy: first random search, then grid seaarch to best tune."
      ],
      "metadata": {
        "id": "lVV-KowyM_9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìÉ Name the hyper`parameters`\n",
        "Once you've picked a `method` to try out new values of the hyperparameters,\n",
        "you need to define what those `parameters` are.\n",
        "\n",
        "Most of the time, this step is straightforward:\n",
        "you just give the `parameter` a name\n",
        "and specify a list of legal `values`\n",
        "of the parameter.\n"
      ],
      "metadata": {
        "id": "_0xgn48KNiJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "        },\n",
        "    'fc_layer_size': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.4, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "WSO1-JbZNhUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's often the case that there are hyperparameters\n",
        "that we don't want to vary in this Sweep,\n",
        "but which we still want to set in our `sweep_config`.\n",
        "\n",
        "In that case, we just set the `value` directly:"
      ],
      "metadata": {
        "id": "VkA9WxWPOZTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 1}\n",
        "    })"
      ],
      "metadata": {
        "id": "q19NWjNcM-WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a `grid` search, that's all you ever need.\n",
        "\n",
        "For a `random` search,\n",
        "all the `values` of a parameter are equally likely to be chosen on a given run.\n",
        "\n",
        "If that just won't do,\n",
        "you can instead specify a named `distribution`,\n",
        "plus its parameters, like the mean `mu`\n",
        "and standard deviation `sigma` of a `normal` distribution.\n",
        "\n",
        "See more on how to set the distributions of your random variables [here](https://docs.wandb.com/sweeps/configuration#distributions)."
      ],
      "metadata": {
        "id": "f9Kgr2PjOl2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#example of how to search parameters from a certain distribution\n",
        "parameters_dict.update({\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 0 and 0.1\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0,\n",
        "        'max': 0.1\n",
        "      },\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms\n",
        "        'distribution': 'q_log_uniform_values',\n",
        "        'q': 8,\n",
        "        'min': 32,\n",
        "        'max': 256,\n",
        "      }\n",
        "    })"
      ],
      "metadata": {
        "id": "sy7JVnqFOvNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we're finished, `sweep_config` is a nested dictionary\n",
        "that specifies exactly which `parameters` we're interested in trying\n",
        "and what `method` we're going to use to try them."
      ],
      "metadata": {
        "id": "Nnenm2z3O9-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe6guob0KdQ-"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRfyjuXjPI16"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}